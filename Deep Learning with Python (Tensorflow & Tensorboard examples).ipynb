{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomado de: \n",
    "https://github.com/Apress/deep-learning-w-python/tree/master/code/chapter11\n",
    "\n",
    "To see the graph in Tensorboard:\n",
    "\n",
    "1) In the terminal type: tensorboard --logdir==training:temp --host=127.0.0.1\n",
    "\n",
    "2) Then. in your browser, opens:http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: ((1 - 2 + 3) * 4)/5.0 =  1.6\n",
      "Via Tensorflow: ((1 - 2 + 3) * 4)/5.0 =  1.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(1.0, name='a')\n",
    "b = tf.constant(2.0, name='b')\n",
    "c = tf.constant(3.0, name='c')\n",
    "d = tf.constant(4.0, name='d')\n",
    "e = tf.constant(5.0, name='e')\n",
    "f = ((a - b + c) * d )/e\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "print (\"Expected: ((1 - 2 + 3) * 4)/5.0 = \", ((1 - 2 + 3) * 4)/5.0)\n",
    "result = sess.run(f)\n",
    "print( \"Via Tensorflow: ((1 - 2 + 3) * 4)/5.0 = \", result)\n",
    "\n",
    "# Expected: ((1 - 2 + 3) * 4)/5.0 =  1.6\n",
    "# Via Tensorflow: ((1 - 2 + 3) * 4)/5.0 =  1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: ((1 - 2 + 3) * 4)/5.0 =  1.6\n",
      "Via Tensorflow: ((1 - 2 + 3) * 4)/5.0 =  1.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(1.0, name='a')\n",
    "b = tf.constant(2.0, name='b')\n",
    "c = tf.constant(3.0, name='c')\n",
    "d = tf.constant(4.0, name='d')\n",
    "e = tf.constant(5.0, name='e')\n",
    "f = ((a - b + c) * d )/e\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "print( \"Expected: ((1 - 2 + 3) * 4)/5.0 = \", ((1 - 2 + 3) * 4)/5.0)\n",
    "result = sess.run(f)\n",
    "print( \"Via Tensorflow: ((1 - 2 + 3) * 4)/5.0 = \", result)\n",
    "\n",
    "# Expected: ((1 - 2 + 3) * 4)/5.0 =  1.6\n",
    "# Via Tensorflow: ((1 - 2 + 3) * 4)/5.0 =  1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: ((1 - 2 + 3) * 4)/5.0 =  1.6\n",
      "Via Tensorflow: ((1 - 2 + 3) * 4)/5.0 =  1.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.placeholder(tf.float32, name='a')\n",
    "b = tf.placeholder(tf.float32, name='b')\n",
    "c = tf.constant(3.0, name='c')\n",
    "d = tf.constant(4.0, name='d')\n",
    "e = tf.constant(5.0, name='e')\n",
    "f = ((a - b + c) * d )/e\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "print(\"Expected: ((1 - 2 + 3) * 4)/5.0 = \", ((1 - 2 + 3) * 4)/5.0)\n",
    "result = sess.run(f,feed_dict={a:1,b:2})\n",
    "print( \"Via Tensorflow: ((1 - 2 + 3) * 4)/5.0 = \", result)\n",
    "\n",
    "# Expected: ((1 - 2 + 3) * 4)/5.0 =  1.6\n",
    "# Via Tensorflow: ((1 - 2 + 3) * 4)/5.0 =  1.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: ((1 - 2 + 3) * 4)/5.0 =  1.6\n",
      "Via Tensorflow: ((1 - 2 + 3) * 4)/5.0 =  1.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.placeholder(tf.float32, name='a')\n",
    "b = tf.placeholder(tf.float32, name='b')\n",
    "c = tf.Variable(initial_value=3.0, name='c')\n",
    "d = tf.Variable(initial_value=4.0, name='d')\n",
    "e = tf.constant(5.0, name='e')\n",
    "f = ((a - b + c) * d )/e\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "print( \"Expected: ((1 - 2 + 3) * 4)/5.0 = \", ((1 - 2 + 3) * 4)/5.0)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "result = sess.run(f,feed_dict={a:1,b:2})\n",
    "print( \"Via Tensorflow: ((1 - 2 + 3) * 4)/5.0 = \", result)\n",
    "\n",
    "# Expected: ((1 - 2 + 3) * 4)/5.0 =  1.6\n",
    "# Via Tensorflow: ((1 - 2 + 3) * 4)/5.0 =  1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: [[-6 -6]\n",
      " [-6 -6]]\n",
      "Via Tensorflow:  [[-6 -6]\n",
      " [-6 -6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "a_data = np.array([[1,1],[1,1]])\n",
    "b_data = np.array([[2,2],[2,2]])\n",
    "c_data = np.array([[5,5],[5,5]])\n",
    "d_data = np.array([[3,3],[3,3]])\n",
    "\n",
    "a = tf.constant([[1,1],[1,1]], name='a')\n",
    "b = tf.constant([[2,2],[2,2]], name='b')\n",
    "c = tf.constant([[5,5],[5,5]], name='c')\n",
    "d = tf.constant([[3,3],[3,3]], name='d')\n",
    "e = (a + b - c) * d\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "print( \"Expected:\", (a_data + b_data - c_data) * d_data)\n",
    "result = sess.run(e)\n",
    "print( \"Via Tensorflow: \", result)\n",
    "\n",
    "# Expected: [[-6 -6]\n",
    "#  [-6 -6]]\n",
    "# Via Tensorflow:  [[-6 -6]\n",
    "#  [-6 -6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: [[-6 -6]\n",
      " [-6 -6]]\n",
      "Via Tensorflow:  [[-6. -6.]\n",
      " [-6. -6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "a_data = np.array([[1,1],[1,1]])\n",
    "b_data = np.array([[2,2],[2,2]])\n",
    "c_data = np.array([[5,5],[5,5]])\n",
    "d_data = np.array([[3,3],[3,3]])\n",
    "\n",
    "a = tf.placeholder(tf.float32, name='a')\n",
    "b = tf.placeholder(tf.float32, name='b')\n",
    "c = tf.placeholder(tf.float32, name='c')\n",
    "d = tf.placeholder(tf.float32, name='d')\n",
    "e = (a + b - c) * d\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "print( \"Expected:\", (a_data + b_data - c_data) * d_data)\n",
    "result = sess.run(e,feed_dict={a:[[1,1],[1,1]],b:[[2,2],[2,2]],c:[[5,5],[5,5]],d:[[3,3],[3,3]]})\n",
    "print( \"Via Tensorflow: \", result)\n",
    "\n",
    "# Expected: [[-6 -6]\n",
    "#  [-6 -6]]\n",
    "# Via Tensorflow:  [[-6 -6]\n",
    "#  [-6 -6]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: [[-6 -6]\n",
      " [-6 -6]]\n",
      "Via Tensorflow:  [[-6. -6.]\n",
      " [-6. -6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "a_data = np.array([[1,1],[1,1]])\n",
    "b_data = np.array([[2,2],[2,2]])\n",
    "c_data = np.array([[5,5],[5,5]])\n",
    "d_data = np.array([[3,3],[3,3]])\n",
    "\n",
    "a = tf.constant([[1.0,1.0],[1.0,1.0]], name='a')\n",
    "b = tf.constant([[2.0,2.0],[2.0,2.0]], name='b')\n",
    "c = tf.placeholder(tf.float32, name='c')\n",
    "d = tf.placeholder(tf.float32, name='d')\n",
    "e = (a + b - c) * d\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "print( \"Expected:\", (a_data + b_data - c_data) * d_data)\n",
    "result = sess.run(e,feed_dict={c:[[5.0,5.0],[5.0,5.0]],d:[[3.0,3.0],[3.0,3.0]]})\n",
    "print( \"Via Tensorflow: \", result)\n",
    "\n",
    "# Expected: [[-6 -6]\n",
    "#  [-6 -6]]\n",
    "# Via Tensorflow:  [[-6 -6]\n",
    "#  [-6 -6]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: [[-6 -6]\n",
      " [-6 -6]]\n",
      "Via Tensorflow:  [[-6. -6.]\n",
      " [-6. -6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "a_data = np.array([[1,1],[1,1]])\n",
    "b_data = np.array([[2,2],[2,2]])\n",
    "c_data = np.array([[5,5],[5,5]])\n",
    "d_data = np.array([[3,3],[3,3]])\n",
    "\n",
    "a = tf.constant([[1.0,1.0],[1.0,1.0]], name='a')\n",
    "b = tf.constant([[2.0,2.0],[2.0,2.0]], name='b')\n",
    "c = tf.Variable(initial_value=[[5.0,5.0],[5.0,5.0]], name='c')\n",
    "d = tf.placeholder(tf.float32, name='d')\n",
    "e = (a + b - c) * d\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "print( \"Expected:\", (a_data + b_data - c_data) * d_data)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "result = sess.run(e,feed_dict={d:[[3.0,3.0],[3.0,3.0]]})\n",
    "print( \"Via Tensorflow: \", result)\n",
    "\n",
    "# Expected: [[-6 -6]\n",
    "#  [-6 -6]]\n",
    "# Via Tensorflow:  [[-6 -6]\n",
    "#  [-6 -6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: [[-26.25 -26.25]\n",
      " [-26.25 -26.25]]\n",
      "Via Tensorflow:  [[-26.25 -26.25]\n",
      " [-26.25 -26.25]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.placeholder(tf.float32, name='a')\n",
    "b = tf.placeholder(tf.float32, name='b')\n",
    "c = tf.Variable(initial_value=[[5.0,5.0],[5.0,5.0]], name='c')\n",
    "d = tf.Variable(initial_value=[[3.0,3.0],[3.0,3.0]], name='d')\n",
    "\n",
    "p = tf.placeholder(tf.float32, name='p')\n",
    "q = tf.placeholder(tf.float32, name='q')\n",
    "r = tf.Variable(initial_value=3.0, name='r')\n",
    "s = tf.Variable(initial_value=4.0, name='s')\n",
    "u = tf.constant(5.0, name='u')\n",
    "e = (((a * p) + (b - q) - (c + r )) * d/s) * u\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "a_data = np.array([[1,1],[1,1]])\n",
    "b_data = np.array([[2,2],[2,2]])\n",
    "c_data = np.array([[5,5],[5,5]])\n",
    "d_data = np.array([[3,3],[3,3]])\n",
    "print( \"Expected:\", (((a_data * 1.0) + (b_data - 2.0) - (c_data + 3.0 )) * d_data/4.0) * 5.0)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "result = sess.run(e,feed_dict={p:1.0, q:2.0, a:[[1,1],[1,1]],b:[[2,2],[2,2]]})\n",
    "print( \"Via Tensorflow: \", result)\n",
    "\n",
    "# Expected: [[-26.25 -26.25]\n",
    "#  [-26.25 -26.25]]\n",
    "# Via Tensorflow:  [[-26.25 -26.25]\n",
    "#  [-26.25 -26.25]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Construct from diagonal\n",
    "a = tf.diag([1.0,1.0,1.0], name=\"a\")\n",
    "\n",
    "# Random Normalised Matrix\n",
    "b = tf.truncated_normal([3,3], name = \"b\")\n",
    "\n",
    "# Simple Fill\n",
    "c = tf.fill([3,4], -1.0, name = \"c\")\n",
    "\n",
    "# Uniform Random\n",
    "d = tf.random_uniform([3,3], name = \"d\")\n",
    "\n",
    "# From Numpy\n",
    "e = tf.convert_to_tensor(np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7., 8.0, 9.0]]), name=\"e\")\n",
    "\n",
    "# Addition\n",
    "f = tf.add(a,b, name=\"f\")\n",
    "\n",
    "# Subtraction\n",
    "g = tf.subtract(a,b, name=\"g\")\n",
    "\n",
    "# Multiplcation\n",
    "h = tf.matmul(a,b, name=\"h\")\n",
    "\n",
    "# Division\n",
    "i = tf.transpose(a, name=\"i\")\n",
    "\n",
    "# Determinant\n",
    "j = tf.matrix_determinant(d, name = \"j\")\n",
    "\n",
    "# Inverse\n",
    "k = tf.matrix_inverse(e, name = \"k\")\n",
    "\n",
    "# Cholesky Decomposition\n",
    "l = tf.cholesky(a, name = \"l\")\n",
    "\n",
    "# Eigen Values and Vectors\n",
    "m = tf.self_adjoint_eig(a, name = \"m\")\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Creating a tensor\n",
    "t1 = tf.zeros([1,20], name=\"t1\")\n",
    "\n",
    "# Creating variables\n",
    "v1 = tf.Variable(t1, name=\"v1\")\n",
    "v2 = tf.Variable(t1, name=\"v2\")\n",
    "\n",
    "# Creating variables based on given dimensions\n",
    "r = 4\n",
    "c = 5\n",
    "t2 = tf.zeros([r,c], name=\"t2\")\n",
    "t3 = tf.ones([r,c], name=\"t3\")\n",
    "v3 = tf.Variable(t2, name=\"v3\")\n",
    "v4 = tf.Variable(t3, name=\"v4\")\n",
    "\n",
    "# Using the shape of a previously defined variable\n",
    "v5 = tf.Variable(tf.zeros_like(v3), name=\"v5\")\n",
    "v6 = tf.Variable(tf.ones_like(v4), name=\"v6\")\n",
    "\n",
    "# Fill Initialization\n",
    "v7 = tf.Variable(tf.fill([r, c], -42), name=\"v7\")\n",
    "\n",
    "# Constant Initialization\n",
    "v8 = tf.Variable(tf.constant([1,2,3,4,5,6,7,8]), name=\"v8\")\n",
    "\n",
    "# Constant Initialization\n",
    "v9 = tf.Variable(tf.constant(42, shape=[r, c]), name=\"v9\")\n",
    "\n",
    "# Linearly spaced Initialization\n",
    "v10 = tf.Variable(tf.linspace(start=-10.0, stop=10.0, num=100), name=\"v10\")\n",
    "\n",
    "# Range Initialization\n",
    "v11 = tf.Variable(tf.range(start=-1.0, limit=1, delta=0.1), name=\"v11\")\n",
    "\n",
    "# Random Normal Initialization\n",
    "v12 = tf.random_normal([r, c], mean=0.0, stddev=1.0, name=\"v12\")\n",
    "\n",
    "# Add the graph for visualization on TensorBoard\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before training: 101.87841\n",
      "Loss after training: 2.0544074\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Generate Random Data\n",
    "examples = 1000\n",
    "features = 100\n",
    "x_data = np.random.randn(examples, features)\n",
    "y_data = np.random.randn(examples,1)\n",
    "\n",
    "# Define the Linear Regression Model\n",
    "X = tf.placeholder(tf.float32, shape=[None, features], name = \"X\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name = \"y\")\n",
    "w = tf.Variable(tf.random_normal(shape=[features,1]), name= \"w\")\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]), name=\"b\")\n",
    "y_hat = tf.add(tf.matmul(X,w),b, name=\"y_hat\")\n",
    "\n",
    "# Define the loss\n",
    "squared_loss = tf.reduce_sum(tf.pow(y - y_hat,2), name=\"squared_loss\")/examples\n",
    "\n",
    "# Set up the gradient descent\n",
    "learning_rate = 0.05\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_step = optimiser.minimize(squared_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epochs = 5000\n",
    "batch_size = 5\n",
    "\n",
    "# Before Training\n",
    "curr_loss = sess.run(squared_loss, feed_dict={X:x_data, y:y_data})\n",
    "print( \"Loss before training:\", curr_loss)\n",
    "\n",
    "for i in range(epochs):\n",
    "    rand_index = np.random.choice(examples, size=batch_size)\n",
    "    sess.run(train_step, feed_dict={X:x_data[rand_index], y:y_data[rand_index]})\n",
    "\n",
    "# After Training\n",
    "curr_loss = sess.run(squared_loss, feed_dict={X:x_data, y:y_data})\n",
    "print( \"Loss after training:\", curr_loss)\n",
    "\n",
    "# Loss before training: 95.5248\n",
    "# Loss after training: 2.13263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before training: 4.79667\n",
      "Loss after training: 0.7021718\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Generate Random Data\n",
    "examples = 1000\n",
    "features = 100\n",
    "x_data = np.random.randn(examples, features)\n",
    "y_data = np.random.randint(size=(examples, 1),low=0, high=2)\n",
    "\n",
    "# Define the Logistic Regression Model\n",
    "X = tf.placeholder(tf.float32, shape=[None, features], name = \"X\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name = \"y\")\n",
    "w = tf.Variable(tf.random_normal(shape=[features,1]), name= \"w\")\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]), name=\"b\")\n",
    "\n",
    "# Loss\n",
    "temp = tf.add(tf.matmul(X, w),b, name=\"temp\")\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=temp, labels=y), name=\"loss\")\n",
    "\n",
    "# Set up the gradient descent\n",
    "learning_rate = 0.05\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_step = optimiser.minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epochs = 5000\n",
    "batch_size = 5\n",
    "\n",
    "# Before Training\n",
    "curr_loss = sess.run(loss, feed_dict={X:x_data, y:y_data})\n",
    "print( \"Loss before training:\", curr_loss)\n",
    "\n",
    "for i in range(epochs):\n",
    "    rand_index = np.random.choice(examples, size=batch_size)\n",
    "    sess.run(train_step, feed_dict={X:x_data[rand_index], y:y_data[rand_index]})\n",
    "\n",
    "# After Training\n",
    "curr_loss = sess.run(loss, feed_dict={X:x_data, y:y_data})\n",
    "print( \"Loss after training:\", curr_loss)\n",
    "\n",
    "# Loss before training: 3.50893\n",
    "# Loss after training: 0.704702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before training: 181.55325\n",
      "Loss after training: 0.9595891\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Generate Random Data\n",
    "examples = 1000\n",
    "features = 100\n",
    "x_data = np.random.randn(examples, features)\n",
    "y_data = np.random.randn(examples,1)\n",
    "\n",
    "# Define the Neural Network Model\n",
    "hidden_layer_nodes = 10\n",
    "X = tf.placeholder(tf.float32, shape=[None, features], name = \"X\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name = \"y\")\n",
    "w1 = tf.Variable(tf.random_normal(shape=[features,hidden_layer_nodes]), name=\"w1\")\n",
    "b1 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes]), name=\"b1\")\n",
    "w2 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes,1]), name=\"w2\")\n",
    "b2 = tf.Variable(tf.random_normal(shape=[1,1]), name=\"b2\")\n",
    "hidden_output = tf.nn.relu(tf.add(tf.matmul(X, w1), b1), name=\"hidden_output\")\n",
    "y_hat = tf.nn.relu(tf.add(tf.matmul(hidden_output, w2), b2), name=\"y_hat\")\n",
    "loss = tf.reduce_mean(tf.square(y_hat - y), name=\"loss\")\n",
    "\n",
    "# Set up the gradient descent\n",
    "learning_rate = 0.05\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_step = optimiser.minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epochs = 5000\n",
    "batch_size = 5\n",
    "\n",
    "# Before Training\n",
    "curr_loss = sess.run(loss, feed_dict={X:x_data, y:y_data})\n",
    "print( \"Loss before training:\", curr_loss)\n",
    "\n",
    "for i in range(epochs):\n",
    "    rand_index = np.random.choice(examples, size=batch_size)\n",
    "    sess.run(train_step, feed_dict={X:x_data[rand_index], y:y_data[rand_index]})\n",
    "\n",
    "# After Training\n",
    "curr_loss = sess.run(loss, feed_dict={X:x_data, y:y_data})\n",
    "print (\"Loss after training:\", curr_loss)\n",
    "\n",
    "# Loss before training: 42.431\n",
    "# Loss after training: 0.976375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./temp\\train-images-idx3-ubyte.gz\n",
      "Extracting ./temp\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./temp\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./temp\\t10k-labels-idx1-ubyte.gz\n",
      "Accuracy before training: 0.117\n",
      "Accuracy after training: 0.886\n"
     ]
    }
   ],
   "source": [
    "# This is a very long exercise, it takes a lot of computational time to complete...\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Helper function to flatten a tensor for convolution and pooling operations\n",
    "def flatten_out(X, height, width, channels,\n",
    "                convolution_output_height_dim,\n",
    "                convolution_output_width_dim,\n",
    "                stride, padding):\n",
    "\n",
    "    # Pad zeros\n",
    "    X_padded = tf.pad(X, [[0,0], [padding, padding], [padding, padding], [0,0]])\n",
    "\n",
    "    # Simulate the sliding of the convolution weights (filter) as a window over the images\n",
    "    slices = []\n",
    "    for i in range(convolution_output_height_dim):\n",
    "        for j in range(convolution_output_width_dim):\n",
    "            window = tf.slice(X_padded, [0, i*stride, j*stride, 0], [-1, height, width, -1])\n",
    "            slices.append(window)\n",
    "\n",
    "    # Combine, reshape and return result\n",
    "    stacked = tf.stack(slices)\n",
    "    return tf.reshape(stacked, [-1, channels * width * height])\n",
    "\n",
    "# Convolution Operation\n",
    "def convolution(X, conv_weights,\n",
    "                conv_bias, padding,\n",
    "                stride, name=\"convolution\"):\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "\n",
    "        # Extract dimensions of input (X)\n",
    "        # and convolution weights\n",
    "        X_filter_count_dim, \\\n",
    "        X_height_dim, \\\n",
    "        X_width_dim, \\\n",
    "        X_channels_dim = [d.value for d in X.get_shape()]\n",
    "\n",
    "        cw_height_dim, \\\n",
    "        cw_width_dim, \\\n",
    "        cw_channels_dim, \\\n",
    "        cw_filter_count_dim = [d.value for d in conv_weights.get_shape()]\n",
    "\n",
    "        # Compute the output dimensions of the\n",
    "        # of the convolution operation on\n",
    "        # X and conv_weights\n",
    "        convolution_output_height_dim =\\\n",
    "            (X_height_dim + 2*padding - cw_height_dim)//stride + 1\n",
    "\n",
    "        convolution_output_width_dim =\\\n",
    "            (X_width_dim + 2*padding - cw_width_dim)//stride + 1\n",
    "\n",
    "        # Flatten X and conv_weights so that a\n",
    "        # matrix mutiplication will lead\n",
    "        # to a convolution operation\n",
    "        X_flattened = flatten_out(X, cw_height_dim,\n",
    "                                  cw_width_dim, cw_channels_dim,\n",
    "                                  convolution_output_height_dim,\n",
    "                                  convolution_output_width_dim,\n",
    "                                  stride, padding)\n",
    "\n",
    "        cw_flattened = tf.reshape(conv_weights, [cw_height_dim *\n",
    "                                                 cw_width_dim *\n",
    "                                                 cw_channels_dim,\n",
    "                                                 cw_filter_count_dim])\n",
    "\n",
    "        # Multiply the flattened matrices\n",
    "        z = tf.matmul(X_flattened, cw_flattened) + conv_bias\n",
    "\n",
    "        # Unflatten/reorganise and return result\n",
    "        return tf.transpose(tf.reshape(z, [convolution_output_height_dim,\n",
    "                                           convolution_output_width_dim,\n",
    "                                           X_filter_count_dim,\n",
    "                                           cw_filter_count_dim]),\n",
    "                            [2, 0, 1, 3])\n",
    "\n",
    "# ReLU operation\n",
    "def relu(X, name = \"relu\"):\n",
    "    with tf.name_scope(name):\n",
    "        return tf.maximum(X, tf.zeros_like(X))\n",
    "\n",
    "# Max Pooling Operation\n",
    "def max_pooling(X, pooling_height, pooling_width,\n",
    "                padding, stride, name =\"pooling\"):\n",
    "    with tf.name_scope(name):\n",
    "        # Get dimensions of input (X)\n",
    "        X_filter_count_dim, \\\n",
    "        X_height_dim, \\\n",
    "        X_width_dim, \\\n",
    "        X_channels_dim = [d.value for d in X.get_shape()]\n",
    "\n",
    "        # Compute the output dimensions of the result\n",
    "        # of the convolution operation on\n",
    "        # X and conv_weights\n",
    "        convolution_output_height_dim = (X_height_dim + 2 * padding - pooling_height) // stride + 1\n",
    "        convolution_output_width_dim = (X_width_dim + 2 * padding - pooling_width) // stride + 1\n",
    "\n",
    "        # Flatten for max operation\n",
    "        X_flattened = flatten_out(X, pooling_height, pooling_width,\n",
    "                                  X_channels_dim,\n",
    "                                  convolution_output_height_dim,\n",
    "                                  convolution_output_width_dim, stride, padding)\n",
    "        # Max Pooling\n",
    "        pool = tf.reduce_max(tf.reshape(X_flattened,\n",
    "                                        [convolution_output_height_dim,\n",
    "                                         convolution_output_width_dim,\n",
    "                                         X_filter_count_dim,\n",
    "                                         pooling_height *\n",
    "                                         pooling_width,\n",
    "                                         X_channels_dim]),\n",
    "                             axis=3)\n",
    "        # Reorg and return result\n",
    "        return tf.transpose(pool, [2, 0, 1, 3])\n",
    "\n",
    "# Fully connected layer\n",
    "def fully_connected(X, W, b, name=\"fully-connected\"):\n",
    "    with tf.name_scope(name):\n",
    "        n = X.get_shape()[0].value\n",
    "        X_flat = tf.reshape(X, [n, -1])\n",
    "        return tf.matmul(X_flat, W) + b\n",
    "\n",
    "# Softmax\n",
    "def softmax(X, name=\"softmax\"):\n",
    "    with tf.name_scope(name):\n",
    "        X_centered = X - tf.reduce_max(X)\n",
    "        X_exp = tf.exp(X_centered)\n",
    "        exp_sum = tf.reduce_sum(X_exp, axis=1)\n",
    "        return tf.transpose(tf.transpose(X_exp) / exp_sum)\n",
    "\n",
    "# Cross Entropy (Loss function for training)\n",
    "def cross_entropy(y, t, name=\"cross-entropy\"):\n",
    "    with tf.name_scope(name):\n",
    "        return -tf.reduce_mean(tf.log(tf.reduce_sum(y * t, axis=1)))\n",
    "\n",
    "# Accuracy (for evalution)\n",
    "def accuracy(network, t, name=\"accuracy\"):\n",
    "    with tf.name_scope(name):\n",
    "        t_predict = tf.argmax(network, axis=1)\n",
    "        t_actual = tf.argmax(t, axis=1)\n",
    "        return tf.reduce_mean(tf.cast(tf.equal(t_predict, t_actual), tf.float32))\n",
    "\n",
    "# Read the input\n",
    "mnist = input_data.read_data_sets(\"./temp\", one_hot=True, reshape=False)\n",
    "\n",
    "# Parameters describing the input data\n",
    "batch_size = 1000\n",
    "image_height = 28\n",
    "image_width = 28\n",
    "image_channels = 1 # monochromatic images\n",
    "categories = 10\n",
    "\n",
    "# Placeholders for input and output\n",
    "X = tf.placeholder(tf.float32, shape=[batch_size, image_height, image_width, image_channels], name = \"X\")\n",
    "y = tf.placeholder(tf.float32, shape=[batch_size, categories], name = \"y\")\n",
    "\n",
    "# Convolution weight parameters\n",
    "conv_height = 7\n",
    "conv_width = 7\n",
    "conv_channels = 1\n",
    "conv_filter_count = 20\n",
    "\n",
    "# Convolution weight and bias\n",
    "convolution_weights = tf.Variable(tf.random_normal([conv_height, conv_width, conv_channels, conv_filter_count], stddev=0.01),\n",
    "                                  name=\"convolution_weights\")\n",
    "convolution_bias = tf.Variable(tf.zeros([conv_filter_count]), name=\"convolution_bias\")\n",
    "\n",
    "# Convolution Layer\n",
    "conv_layer = convolution(X, convolution_weights, convolution_bias, padding=2, stride=1, name=\"Convolution\")\n",
    "\n",
    "# Convolution Layer Activation\n",
    "conv_activation_layer = relu(conv_layer,name=\"convolution_actiavation_relu\")\n",
    "\n",
    "# Pooling Layer\n",
    "pooling_layer = max_pooling(conv_activation_layer, pooling_height=2, pooling_width=2, padding=0, stride=2, name =\"Pooling\")\n",
    "\n",
    "# Fully Connected Layer-1 (Hidden Layer)\n",
    "hidden_size = 150\n",
    "batch_size, pool_output_h, pool_output_w, conv_filter_count = [d.value for d in pooling_layer.get_shape()]\n",
    "weights1 = tf.Variable(tf.random_normal([pool_output_h * pool_output_w * conv_filter_count, hidden_size], stddev=0.01),\n",
    "                       name=\"weights1\")\n",
    "bias1 = tf.Variable(tf.zeros([hidden_size]), name=\"bias1\")\n",
    "fully_connected1 = fully_connected(pooling_layer, weights1, bias1, name=\"Fully-Connected-1\")\n",
    "fully_connected1_activation = relu(fully_connected1, name=\"fully_connected1_activation_relu\")\n",
    "\n",
    "# Fully Connected Layer-2 (Output Layer)\n",
    "output_size = 10\n",
    "weights2 = tf.Variable(tf.random_normal([hidden_size, output_size], stddev=0.01), name=\"weights2\")\n",
    "bias2 = tf.Variable(tf.zeros([output_size]), name=\"bias2\")\n",
    "fully_connected2 = fully_connected(fully_connected1_activation, weights2, bias2, name=\"Fully-Connected-2\")\n",
    "\n",
    "# Softmax\n",
    "softmax_layer = softmax(fully_connected2, name=\"Softmax\")\n",
    "\n",
    "# Cross Entropy Loss\n",
    "loss = cross_entropy(softmax_layer, y, name =\"Cross-Entropy\")\n",
    "\n",
    "# Training and Evaluation\n",
    "learning_rate = 0.1\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Before Training\n",
    "test_x = mnist.test.images[:batch_size]\n",
    "test_y = mnist.test.labels[:batch_size]\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print( \"Accuracy before training:\", sess.run(accuracy(softmax_layer, y), feed_dict={X:test_x, y:test_y}))\n",
    "batches = int(mnist.train.num_examples/batch_size)\n",
    "steps = 5\n",
    "for i in range(steps):\n",
    "    for j in range(batches):\n",
    "        x_data, y_data = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train_step, feed_dict={X:x_data, y:y_data})\n",
    "print( \"Accuracy after training:\", sess.run(accuracy(softmax_layer, y), feed_dict={X:test_x, y:test_y}))\n",
    "\n",
    "# Accuracy before training: 0.124\n",
    "# Accuracy after training: 0.894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'x_6' with dtype float\n\t [[Node: x_6 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'x_6', defined at:\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-04e872da7a70>\", line 5, in <module>\n    x = tf.placeholder(tf.float32, name=\"x\")\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1735, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 5928, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'x_6' with dtype float\n\t [[Node: x_6 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'x_6' with dtype float\n\t [[Node: x_6 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-d3fcde184a4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m   \u001b[0mtrain_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmerged_summary\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m   \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1289\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1291\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'x_6' with dtype float\n\t [[Node: x_6 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'x_6', defined at:\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-04e872da7a70>\", line 5, in <module>\n    x = tf.placeholder(tf.float32, name=\"x\")\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1735, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 5928, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\ivonnics\\Anaconda3\\envs\\Machinelearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'x_6' with dtype float\n\t [[Node: x_6 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# This is by far, the best example of what Tensorflow can display:\n",
    "# Scalars\n",
    "# Graphs\n",
    "# Distributions and...\n",
    "# Histograms\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "W = tf.Variable([.3], tf.float32, name=\"W\")\n",
    "b = tf.Variable([-.3], tf.float32, name = \"b\")\n",
    "x = tf.placeholder(tf.float32, name=\"x\")\n",
    "with tf.name_scope(\"linear_model\"):\n",
    "    linear_model = W * x + b\n",
    "\n",
    "    # Tensorboard Histogram Summary of W and b\n",
    "    tf.summary.histogram(\"W\", W)\n",
    "    tf.summary.histogram(\"b\", b)\n",
    "\n",
    "y = tf.placeholder(tf.float32, name=\"y\")\n",
    "with tf.name_scope(\"loss_computation\"):\n",
    "    loss = tf.reduce_sum(tf.square(linear_model - y))\n",
    "\n",
    "    # Tensorboard Scalar Summaries for Loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "x_train = [1,2,3,4]\n",
    "y_train = [0,-1,-2,-3]\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Merge Summaries\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Graph Summary\n",
    "writer = tf.summary.FileWriter(\"./temp\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for i in range(1000):\n",
    "  train_result, summary_result = sess.run([train,merged_summary], {x:x_train, y:y_train})\n",
    "  writer.add_summary(summary_result, i)\n",
    "\n",
    "curr_W, curr_b, curr_loss  = sess.run([W, b, loss], {x:x_train, y:y_train})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))\n",
    "\n",
    "# W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
